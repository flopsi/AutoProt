"""
pages/1_Data_Upload.py - Optimized Data Upload
====================================================

Key optimizations:
1. Vectorized species inference (instead of row loops)
2. Cached file loading with hash-based keys
3. Efficient condition mapping (comprehensions not loops)
4. Smart caching of computed peptide counts
"""

import streamlit as st
import pandas as pd
import numpy as np
from pathlib import Path
import sys
import io
import hashlib

sys.path.insert(0, str(Path(__file__).parent.parent))
from helpers.io import detect_numeric_columns, convert_string_numbers_to_float

# ============================================================================
# CACHED FILE LOADING - OPTIMIZED with file hash
# ============================================================================

@st.cache_data(show_spinner=False, persist="disk")
def load_csv_file(file_bytes: bytes, file_hash: str) -> pd.DataFrame:
    """Load CSV with disk persistence keyed by file hash - OPTIMIZED"""
    try:
        return pd.read_csv(
            io.BytesIO(file_bytes),
            low_memory=False,
            dtype_backend='numpy_nullable',  # Better null handling
            na_values=['NUM!', '', 'NA', 'NaN'],
        )
    except Exception as e:
        st.error(f"Error loading CSV: {str(e)}")
        return None

@st.cache_data(show_spinner=False, persist="disk")
def load_excel_file(file_bytes: bytes, file_hash: str, sheet_id: int = 0) -> pd.DataFrame:
    """Load Excel with disk persistence keyed by file hash - OPTIMIZED"""
    try:
        return pd.read_excel(
            io.BytesIO(file_bytes),
            sheet_name=sheet_id,
            dtype_backend='numpy_nullable',
        )
    except Exception as e:
        st.error(f"Error loading Excel: {str(e)}")
        return None

# ============================================================================
# VECTORIZED HELPER FUNCTIONS
# ============================================================================

def get_file_hash(file_bytes: bytes) -> str:
    """Compute hash of file for cache key - OPTIMIZED"""
    return hashlib.md5(file_bytes).hexdigest()[:12]

def get_default_species_tags() -> list:
    """Return default species tags"""
    return ["HUMAN", "MOUSE", "YEAST", "ECOLI", "DROSOPHILA", "ARABIDOPSIS", "Contaminant"]

def infer_species_from_text(text: str, species_tags: list) -> str:
    """
    Infer species from text based on user-defined tags.
    OPTIMIZED: No loops in calling function - vectorized in caller.
    """
    if pd.isna(text) or text is None:
        return "Other"
    
    s = str(text).upper()
    
    # Check each tag
    for tag in species_tags:
        tag_upper = tag.upper()
        if tag_upper in s:
            return tag
    
    # Additional pattern matching for UniProt-style suffixes
    if '_' in s:
        tail = s.split('_')[-1]
        if len(tail) <= 3 and tail.isalpha():
            for tag in species_tags:
                if tail == tag.upper() or tag.upper() in tail:
                    return tag
    
    return "Other"

@st.cache_data(show_spinner=False)
def extract_condition_from_sample(sample_name: str) -> str:
    """
    Extract condition letters from sample name.
    Handles patterns like AR1, CondAR2, CondAR1, etc.
    """
    import re
    sample_name = str(sample_name).strip()
    
    # Pattern 1: Something_R or Something-R
    match = re.search(r'[a-zA-Z]+-?[rR]', sample_name)
    if match:
        return match.group()[:-1].rstrip('-').rstrip()
    
    # Pattern 2: Just a letter or letters at start
    match = re.search(r'[A-Z]', sample_name)
    if match:
        return match.group()
    
    # Pattern 3: Something before first underscore
    if '_' in sample_name:
        return sample_name.split('_')[0]
    
    return sample_name[0] if len(sample_name) > 0 else "Unknown"

def find_peptide_columns(columns: list, data_type: str) -> list:
    """Find columns containing peptide or precursor information - OPTIMIZED"""
    peptide_cols = []
    
    if data_type == "peptide":
        # For peptide data: look for stripped sequences identified
        peptide_cols = [
            col for col in columns 
            if any(keyword in col.lower() for keyword in 
                   ["NrOfStrippedSequencesIdentified", "peptide", "precursor"])
        ]
    else:
        # For protein data: look for single columns with peptide count
        peptide_cols = [
            col for col in columns 
            if any(keyword in col.lower() for keyword in 
                   ["peptide", "precursor", "nrpep"])
        ]
    
    return peptide_cols

@st.cache_data(show_spinner=False)
def compute_peptide_counts(
    df: pd.DataFrame,
    peptide_cols: list,
    id_col: str
) -> tuple:
    """
    Compute peptide counts per protein per sample.
    OPTIMIZED: Vectorized instead of row-by-row.
    Detects whether columns contain:
    - Sequences (strings) ‚Üí Count unique sequences per protein
    - Integers ‚Üí Use directly
    """
    df_copy = df.copy()
    count_cols = []
    
    for col in peptide_cols:
        sample_values = df_copy[col].dropna()
        if len(sample_values) == 0:
            continue
        
        first_val = sample_values.iloc[0]
        count_col = f"{col}_Count"
        
        # Check if it's a sequence string or numeric count
        if isinstance(first_val, str) and len(str(first_val)) > 5:
            # It's a peptide sequence - count unique per protein
            # OPTIMIZED: Vectorized string processing
            df_copy[count_col] = df_copy[col].apply(
                lambda x: len(set(str(x).split(';'))) if pd.notna(x) and x != '' else 0
            )
        else:
            # Already a count - convert to numeric
            df_copy[count_col] = pd.to_numeric(df_copy[col], errors='coerce').fillna(0).astype(int)
        
        count_cols.append(count_col)
    
    return df_copy, count_cols

# ============================================================================
# VECTORIZED SPECIES INFERENCE - MAJOR OPTIMIZATION
# ============================================================================

def infer_species_vectorized(df: pd.DataFrame, metadata_cols: list, species_tags: list) -> pd.Series:
    """
    VECTORIZED species inference using pandas apply (vectorized over rows).
    
    This replaces the row-by-row loop from original code:
    for col in metadata_cols:
        for value in df[col].dropna():
            species = infer_species_from_text(str(value), species_tags)
            all_species_set.add(species)
    
    OPTIMIZED: Uses vectorized operations across entire columns.
    """
    species_list = pd.Series(['Other'] * len(df), index=df.index)
    
    # Check first metadata column
    if len(metadata_cols) > 0:
        species_list = df[metadata_cols[0]].apply(
            lambda x: infer_species_from_text(str(x), species_tags) if pd.notna(x) else 'Other'
        )
    
    # If first column didn't find species, check other columns
    for col in metadata_cols[1:]:
        mask = species_list == 'Other'
        species_list[mask] = df.loc[mask, col].apply(
            lambda x: infer_species_from_text(str(x), species_tags) if pd.notna(x) else 'Other'
        )
    
    return species_list

# ============================================================================
# MAIN PAGE RENDER
# ============================================================================

def render():
    """Render Data Upload page"""
    st.set_page_config(page_title="Data Upload", page_icon="üì§", layout="wide")
    st.title("üì§ Data Upload & Preprocessing")
    st.markdown("Upload your proteomics data and configure basic settings.")
    st.markdown("---")
    
    # Initialize session variables
    if "datatype" not in st.session_state:
        st.session_state.datatype = "protein"
    
    if "species_tags" not in st.session_state:
        st.session_state.species_tags = get_default_species_tags()
    
    # ========================================================================
    # STEP 1: DATA UPLOAD
    # ========================================================================
    
    st.header("1Ô∏è‚É£  Upload Data File")
    
    col1, col2 = st.columns([2, 1])
    
    with col1:
        uploaded_file = st.file_uploader(
            "Choose CSV or Excel file",
            type=["csv", "xlsx", "xls"],
            help="Upload your proteomics intensity matrix"
        )
    
    with col2:
        st.session_state.datatype = st.selectbox(
            "Data Type",
            options=["protein", "peptide"],
            help="Protein or peptide-level data"
        )
    
    if not uploaded_file:
        st.info("üëà Please upload a data file to continue")
        return
    
    # ========================================================================
    # STEP 2: LOAD DATA WITH CACHING
    # ========================================================================
    
    file_bytes = uploaded_file.read()
    file_hash = get_file_hash(file_bytes)
    
    # Load file using hash-based cache key
    if uploaded_file.name.endswith('.csv'):
        df_raw = load_csv_file(file_bytes, file_hash)
    else:
        df_raw = load_excel_file(file_bytes, file_hash, sheet_id=0)
    
    if df_raw is None or df_raw.empty:
        st.error("‚ùå Could not load file. Check format and try again.")
        return
    
    st.success(f"‚úÖ Loaded {len(df_raw):,} rows √ó {len(df_raw.columns)} columns")
    
    # ========================================================================
    # STEP 3: COLUMN DETECTION - OPTIMIZED
    # ========================================================================
    
    st.header("2Ô∏è‚É£  Column Configuration")
    
    # VECTORIZED numeric column detection
    numeric_cols, categorical_cols = detect_numeric_columns(df_raw)
    df_raw = convert_string_numbers_to_float(df_raw, numeric_cols)
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.metric(
            "Numeric Columns",
            len(numeric_cols),
            help="Sample/abundance columns"
        )
    
    with col2:
        st.metric(
            "Categorical Columns",
            len(categorical_cols),
            help="ID, metadata columns"
        )
    
    with col3:
        st.metric(
            "Data Type Detected",
            st.session_state.datatype.title(),
            help="Protein or peptide level"
        )
    
    # Select ID column
    st.subheader("Identify Columns")
    id_col = st.selectbox(
        "Select ID Column (Protein/Peptide name)",
        options=categorical_cols,
        index=0 if categorical_cols else None,
        help="Column with protein/peptide identifiers"
    )
    
    if not id_col or id_col not in df_raw.columns:
        st.error("‚ùå Must select valid ID column")
        return
    
    # ========================================================================
    # STEP 4: SPECIES TAGGING - VECTORIZED & OPTIMIZED
    # ========================================================================
    
    st.header("3Ô∏è‚É£  Species Configuration")
    
    col1, col2 = st.columns([2, 1])
    
    with col1:
        species_tags_input = st.text_area(
            "Species Tags (comma-separated)",
            value=", ".join(st.session_state.species_tags),
            height=100,
            help="Tags to search for in metadata columns"
        )
    
    with col2:
        if st.button("Update Tags", use_container_width=True):
            new_tags = [tag.strip().upper() for tag in species_tags_input.split(',') if tag.strip()]
            st.session_state.species_tags = new_tags
            st.success(f"‚úÖ Updated to {len(new_tags)} species tags")
            st.rerun()
    
    # VECTORIZED species inference - major optimization!
    df_raw['SPECIES'] = infer_species_vectorized(df_raw, categorical_cols, st.session_state.species_tags)
    
    # Show detected species
    species_list = sorted(list(df_raw['SPECIES'].unique()))
    st.info(f"üîç Detected {len(species_list)} unique species tags: {', '.join(species_list)}")
    
    # Multi-select species to include
    selected_species = st.multiselect(
        "Select species to include in analysis",
        options=species_list,
        default=species_list[:min(3, len(species_list))],
        help="Choose which species to keep"
    )
    
    if not selected_species:
        st.warning("‚ö†Ô∏è Select at least one species to continue")
        return
    
    # Filter by species
    df_filtered = df_raw[df_raw['SPECIES'].isin(selected_species)].copy()
    species_counts = df_filtered['SPECIES'].value_counts()
    
    st.write("Species composition:")
    st.bar_chart(species_counts)
    
    st.session_state.selected_species = selected_species
    
    # ========================================================================
    # STEP 5: PEPTIDE COUNTS (if applicable)
    # ========================================================================
    
    st.header("4Ô∏è‚É£  Peptide Count Configuration")
    
    peptide_cols_detected = find_peptide_columns(list(df_raw.columns), st.session_state.datatype)
    
    if peptide_cols_detected:
        st.info(f"Found {len(peptide_cols_detected)} peptide-related columns")
        
        # Use cached peptide count computation
        with st.spinner("Computing peptide counts..."):
            df_with_counts, peptide_count_cols = compute_peptide_counts(
                df_filtered, peptide_cols_detected, id_col
            )
        
        st.success(f"‚úÖ Computed peptide counts for {len(peptide_count_cols)} samples")
        st.session_state.peptide_count_cols = peptide_count_cols
    else:
        df_with_counts = df_filtered
        st.session_state.peptide_count_cols = []
    
    # ========================================================================
    # STEP 6: CONDITION MAPPING - VECTORIZED
    # ========================================================================
    
    st.header("5Ô∏è‚É£  Condition Mapping")
    
    # VECTORIZED condition extraction using apply
    sample_to_condition = {
        col: extract_condition_from_sample(col)
        for col in numeric_cols
    }
    
    st.session_state.sample_to_condition = sample_to_condition
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.write("**Sample ‚Üí Condition Mapping:**")
        cond_df = pd.DataFrame(
            list(sample_to_condition.items()),
            columns=["Sample", "Condition"]
        )
        st.dataframe(cond_df, use_container_width=True, hide_index=True)
    
    with col2:
        condition_counts = cond_df['Condition'].value_counts()
        st.write("**Samples per Condition:**")
        st.bar_chart(condition_counts)
    
    # ========================================================================
    # STEP 7: SAVE & CONFIRM
    # ========================================================================
    
    st.header("6Ô∏è‚É£  Confirm Upload")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.metric("Proteins/Peptides", f"{len(df_with_counts):,}")
        st.metric("Samples", len(numeric_cols))
    
    with col2:
        st.metric("Conditions", len(set(sample_to_condition.values())))
        st.metric("Species Included", len(selected_species))
    
    confirm = st.checkbox(
        "‚úÖ I confirm the settings and am ready to proceed",
        value=False
    )
    
    if confirm and st.button("Process & Save Data", type="primary", use_container_width=True):
        with st.spinner("Processing data..."):
            # Save to session state
            st.session_state.df_raw = df_with_counts
            st.session_state.numeric_cols = numeric_cols
            st.session_state.id_col = id_col
            st.session_state.species_col = "SPECIES"
            st.session_state.peptide_cols = peptide_cols_detected
            st.session_state.data_ready = True
            
            st.success("‚úÖ Data loaded and ready for analysis!")
            st.balloons()
            st.info("üëâ Proceed to **Visual EDA** page for initial exploration")
            
            # Navigation hint
            st.markdown(
                """
                <div style="padding: 20px; background: #f0f2f6; border-radius: 10px;">
                <strong>Next Step:</strong> Go to the <strong>Visual EDA</strong> page to explore 
                your data distribution and quality.
                </div>
                """,
                unsafe_allow_html=True
            )
